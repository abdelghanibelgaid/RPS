{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile mpa.py\nimport random\nimport numpy as np\nimport re\n\ndef evaluate_pattern_efficiency(previous_step_result):\n    \"\"\" \n        evaluate efficiency of the pattern and, if pattern is inefficient,\n        remove it from agent's memory\n    \"\"\"\n    pattern_group_index = previous_action[\"pattern_group_index\"]\n    pattern_index = previous_action[\"pattern_index\"]\n    pattern = groups_of_memory_patterns[pattern_group_index][\"memory_patterns\"][pattern_index]\n    pattern[\"reward\"] += previous_step_result\n    # if pattern is inefficient\n    if pattern[\"reward\"] <= EFFICIENCY_THRESHOLD:\n        # remove pattern from agent's memory\n        del groups_of_memory_patterns[pattern_group_index][\"memory_patterns\"][pattern_index]\n    \ndef find_action(group, group_index):\n    \"\"\" if possible, find my_action in this group of memory patterns \"\"\"\n    if len(current_memory) > group[\"memory_length\"]:\n        this_step_memory = current_memory[-group[\"memory_length\"]:]\n        memory_pattern, pattern_index = find_pattern(group[\"memory_patterns\"], this_step_memory, group[\"memory_length\"])\n        if memory_pattern != None:\n            my_action_amount = 0\n            for action in memory_pattern[\"opp_next_actions\"]:\n                # if this opponent's action occurred more times than currently chosen action\n                # or, if it occured the same amount of times and this one is choosen randomly among them\n                if (action[\"amount\"] > my_action_amount or\n                        (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                    my_action_amount = action[\"amount\"]\n                    my_action = action[\"response\"]\n            return my_action, pattern_index\n    return None, None\n\ndef find_pattern(memory_patterns, memory, memory_length):\n    \"\"\" find appropriate pattern and its index in memory \"\"\"\n    for i in range(len(memory_patterns)):\n        actions_matched = 0\n        for j in range(memory_length):\n            if memory_patterns[i][\"actions\"][j] == memory[j]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return memory_patterns[i], i\n    # appropriate pattern not found\n    return None, None\n\ndef get_step_result_for_my_agent(my_agent_action, opp_action):\n    \"\"\" \n        get result of the step for my_agent\n        1, 0 and -1 representing win, tie and lost results of the game respectively\n        reward will be taken from observation in the next release of kaggle environments\n    \"\"\"\n    if my_agent_action == opp_action:\n        return 0\n    elif (my_agent_action == (opp_action + 1)) or (my_agent_action == 0 and opp_action == 2):\n        return 1\n    else:\n        return -1\n    \ndef update_current_memory(obs, my_action):\n    \"\"\" add my_agent's current step to current_memory \"\"\"\n    # if there's too many actions in the current_memory\n    if len(current_memory) > current_memory_max_length:\n        # delete first two elements in current memory\n        # (actions of the oldest step in current memory)\n        del current_memory[:2]\n    # add agent's last action to agent's current memory\n    current_memory.append(my_action)\n    \ndef update_memory_pattern(obs, group):\n    \"\"\" if possible, update or add some memory pattern in this group \"\"\"\n    # if length of current memory is suitable for this group of memory patterns\n    if len(current_memory) > group[\"memory_length\"]:\n        # get memory of the previous step\n        # considering that last step actions of both agents are already present in current_memory\n        previous_step_memory = current_memory[-group[\"memory_length\"] - 2 : -2]\n        previous_pattern, pattern_index = find_pattern(group[\"memory_patterns\"], previous_step_memory, group[\"memory_length\"])\n        if previous_pattern == None:\n            previous_pattern = {\n                # list of actions of both players\n                \"actions\": previous_step_memory.copy(),\n                # total reward earned by using this pattern\n                \"reward\": 0,\n                # list of observed opponent's actions after each occurrence of this pattern\n                \"opp_next_actions\": [\n                    # action that was made by opponent,\n                    # amount of times that action occurred,\n                    # what should be the response of my_agent\n                    {\"action\": 0, \"amount\": 0, \"response\": 1},\n                    {\"action\": 1, \"amount\": 0, \"response\": 2},\n                    {\"action\": 2, \"amount\": 0, \"response\": 0}\n                ]\n            }\n            group[\"memory_patterns\"].append(previous_pattern)\n        # update previous_pattern\n        for action in previous_pattern[\"opp_next_actions\"]:\n            if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                action[\"amount\"] += 1\n\n# maximum steps in a memory pattern\nSTEPS_MAX = 10\n# minimum steps in a memory pattern\nSTEPS_MIN = 5\n# lowest efficiency threshold of a memory pattern before being removed from agent's memory\nEFFICIENCY_THRESHOLD = -5\n# amount of steps between forced random actions\nFORCED_RANDOM_ACTION_INTERVAL = random.randint(STEPS_MIN, STEPS_MAX)\n\n# current memory of the agent\ncurrent_memory = []\n# previous action of my_agent\nprevious_action = {\n    \"action\": None,\n    # action was taken from pattern\n    \"action_from_pattern\": False,\n    \"pattern_group_index\": None,\n    \"pattern_index\": None\n}\n# amount of steps remained until next forced random action\nsteps_to_random = FORCED_RANDOM_ACTION_INTERVAL\n# maximum length of current_memory\ncurrent_memory_max_length = STEPS_MAX * 2\n# current reward of my_agent\n# will be taken from observation in the next release of kaggle environments\nreward = 0\n# memory length of patterns in first group\n# STEPS_MAX is multiplied by 2 to consider both my_agent's and opponent's actions\ngroup_memory_length = current_memory_max_length\n# list of groups of memory patterns\ngroups_of_memory_patterns = []\nfor i in range(STEPS_MAX, STEPS_MIN - 1, -1):\n    groups_of_memory_patterns.append({\n        # how many steps in a row are in the pattern\n        \"memory_length\": group_memory_length,\n        # list of memory patterns\n        \"memory_patterns\": []\n    })\n    group_memory_length -= 2\n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    # action of my_agent\n    my_action = None\n    \n    # forced random action\n    global steps_to_random\n    steps_to_random -= 1\n    if steps_to_random <= 0:\n        steps_to_random = FORCED_RANDOM_ACTION_INTERVAL\n        # choose action randomly\n        my_action = random.randint(0,2)\n        # save action's data\n        previous_action[\"action\"] = my_action\n        previous_action[\"action_from_pattern\"] = False\n        previous_action[\"pattern_group_index\"] = None\n        previous_action[\"pattern_index\"] = None\n    \n    # if it's not first step\n    if obs[\"step\"] > 0:\n        # add opponent's last step to current_memory\n        current_memory.append(obs[\"lastOpponentAction\"])\n        # previous step won or lost\n        previous_step_result = get_step_result_for_my_agent(current_memory[-2], current_memory[-1])\n        global reward\n        reward += previous_step_result\n        # if previous action of my_agent was taken from pattern\n        if previous_action[\"action_from_pattern\"]:\n            evaluate_pattern_efficiency(previous_step_result)\n    \n    for i in range(len(groups_of_memory_patterns)):\n        # if possible, update or add some memory pattern in this group\n        update_memory_pattern(obs, groups_of_memory_patterns[i])\n        # if action was not yet found\n        if my_action == None:\n            my_action, pattern_index = find_action(groups_of_memory_patterns[i], i)\n            if my_action != None:\n                # save action's data\n                previous_action[\"action\"] = my_action\n                previous_action[\"action_from_pattern\"] = True\n                previous_action[\"pattern_group_index\"] = i\n                previous_action[\"pattern_index\"] = pattern_index\n    \n    # if no action was found\n    if my_action == None:\n        # choose action randomly\n        my_action = random.randint(0,2)\n        # save action's data\n        previous_action[\"action\"] = my_action\n        previous_action[\"action_from_pattern\"] = False\n        previous_action[\"pattern_group_index\"] = None\n        previous_action[\"pattern_index\"] = None\n    \n    # add my_agent's current step to current_memory\n    update_current_memory(obs, my_action)\n    return my_action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\nimport random\n\nenv = make(\"rps\", configuration={\"episodeSteps\": 100}, debug=True)\nenv.run([\"mpa.py\", lambda obs, conf: random.randint(0, 2)])\nenv.render(mode=\"ipython\", width=600, height=600)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}